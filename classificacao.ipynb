{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP+57AS7nuPF4ai2zdLMYo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srdiegorodrigues/projeto/blob/main/classificacao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b450uN7Z-ZuT",
        "outputId": "8e393fa4-ffd7-4e73-f850-88c08aceacdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importação das bibliotecas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "import io\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "baqzux-eAm3T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Carregar arquivos a serem analisados\n",
        "url = \"/content/drive/MyDrive/MESTRADO/DADOS/OCORRENCIAS/\"\n",
        "\n",
        "df_2017 = pd.read_csv(io.StringIO(open(f'{url}datatran2017.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
        "df_2018 = pd.read_csv(io.StringIO(open(f'{url}datatran2018.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
        "df_2019 = pd.read_csv(io.StringIO(open(f'{url}datatran2019.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
        "df_2020 = pd.read_csv(io.StringIO(open(f'{url}datatran2020.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
        "df_2021 = pd.read_csv(io.StringIO(open(f'{url}datatran2021.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
        "df_2022 = pd.read_csv(io.StringIO(open(f'{url}datatran2022.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
        "\n",
        "df_2017 = df_2017.dropna()\n",
        "df_2018 = df_2018.dropna()\n",
        "df_2019 = df_2019.dropna()\n",
        "df_2020 = df_2020.dropna()\n",
        "df_2021 = df_2021.dropna()\n",
        "df_2022 = df_2022.dropna()\n",
        "\n",
        "dfs = [df_2017, df_2018, df_2019, df_2020, df_2021, df_2022]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-iV6UCqAoGq",
        "outputId": "ea35124e-375a-4d0b-e26e-598523a54476"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-5849736c2be4>:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_2017 = pd.read_csv(io.StringIO(open(f'{url}datatran2017.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
            "<ipython-input-9-5849736c2be4>:5: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_2018 = pd.read_csv(io.StringIO(open(f'{url}datatran2018.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
            "<ipython-input-9-5849736c2be4>:6: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_2019 = pd.read_csv(io.StringIO(open(f'{url}datatran2019.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
            "<ipython-input-9-5849736c2be4>:7: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_2020 = pd.read_csv(io.StringIO(open(f'{url}datatran2020.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
            "<ipython-input-9-5849736c2be4>:8: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_2021 = pd.read_csv(io.StringIO(open(f'{url}datatran2021.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
            "<ipython-input-9-5849736c2be4>:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_2022 = pd.read_csv(io.StringIO(open(f'{url}datatran2022.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#carregar base de dados dos feriados nacionais\n",
        "df_feriado = pd.read_csv(\"/content/drive/MyDrive/MESTRADO/DADOS/OCORRENCIAS/feriados_nacionais.csv\",  encoding='ISO-8859-1', sep=\";\")\n",
        "\n",
        "#arredondar o valor de km para cima ou para baixo, conforme o valor após o ponto\n",
        "def arredondar_km(valor):\n",
        "    valor_float = float(valor)  # Converter para float\n",
        "    inteiro = int(valor_float)  # parte inteira do valor\n",
        "    decimal = valor_float - inteiro  # parte decimal do valor\n",
        "\n",
        "    # Se o valor decimal for maior ou igual a 0.5, arredonda para cima, caso contrário, arredonda para baixo\n",
        "    if decimal >= 0.5:\n",
        "        return inteiro + 1\n",
        "    else:\n",
        "        return inteiro\n",
        "\n",
        "#converter o valor da feature br para int\n",
        "def br_int(valor):\n",
        "    valor_float = float(valor)\n",
        "    valor_int = int(valor_float)\n",
        "    return valor_int\n",
        "\n",
        "dias_semana_para_int = {\n",
        "        'segunda-feira': 0,\n",
        "        'terça-feira': 1,\n",
        "        'quarta-feira': 2,\n",
        "        'quinta-feira': 3,\n",
        "        'sexta-feira': 4,\n",
        "        'sábado': 5,\n",
        "        'domingo': 6\n",
        "    }\n",
        "\n",
        "def converter_dia_semana_para_int(dia):\n",
        "    return dias_semana_para_int[dia]\n",
        "\n",
        "for df in dfs:\n",
        "    #alterar condições metereologicas para evitar outliers\n",
        "    df['condicao_metereologica'] = df['condicao_metereologica'].str.replace('Ignorado', 'Céu Claro')\n",
        "    df['condicao_metereologica'] = df['condicao_metereologica'].str.replace('Neve', 'Chuva')\n",
        "\n",
        "    #substituir / por - na data_inversa\n",
        "    df['data_inversa'] = df['data_inversa'].str.replace('/', '-')\n",
        "\n",
        "    #criação da feature chuva para determinar se a pista estava molhada na hora do acidente\n",
        "    for indice, item in df.iterrows():\n",
        "        if item['condicao_metereologica'] in ['Chuva', 'Garoa/Chuvisco', 'Granizo']:\n",
        "            df.at[indice, 'chuva'] = 1\n",
        "        else:\n",
        "            df.at[indice, 'chuva'] = 0\n",
        "\n",
        "    #fragmentação da feature data_inversa em dia, mes e ano\n",
        "    df[['dia', 'mes', 'ano']] = df['data_inversa'].apply(lambda x: pd.Series(x.split('-')).astype(int))\n",
        "\n",
        "    # #Concatenar colunas data_inversa e horario e criando a feature data_hora\n",
        "    df['data_hora'] = df.apply(lambda x: '%s-%s' % (x['data_inversa'],x['horario']), axis=1)\n",
        "    #Criação da feature data_hora_int\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    # Convertendo as datas para timestamps Unix\n",
        "    df['data_hora_int'] = df['data_hora'].astype(int) // 10**9\n",
        "\n",
        "    #normalizar a feature data_hora_int\n",
        "    scaler = MinMaxScaler()\n",
        "    df['data_hora_int'] = scaler.fit_transform(df['data_hora_int'].values.reshape(-1, 1))\n",
        "\n",
        "    #substituir a , por . na feature km\n",
        "    df['km'] = df['km'].str.replace(',','.')\n",
        "    df['km'] = df['km'].apply(arredondar_km)\n",
        "    df['br'] = df['br'].apply(br_int)\n",
        "\n",
        "\n",
        "    df['dia_semana_int'] =  df['dia_semana'].apply(converter_dia_semana_para_int)\n",
        "\n",
        "    #Setar se dia da ocorrência do acidente era feriado ou não\n",
        "    #Criando a feature feriado com base nas portarias do governo federal\n",
        "    #Caso seja feriado, será atribuido o valor 1 para o registro\n",
        "    #caso contrário, será atribuido o valor 0\n",
        "    df['data_inversa'] = pd.to_datetime(df['data_inversa'])\n",
        "    df_feriado['Data'] = pd.to_datetime(df_feriado['Data'])\n",
        "    df_feriado['Data'] = df_feriado['Data'].sort_values(ascending=False).values\n",
        "    # Criando uma nova coluna 'feriado' no DataFrame principal e inicializando com 0\n",
        "    df['feriado'] = 0\n",
        "\n",
        "    # Loop for para comparar as datas e atribuir 1 se forem iguais\n",
        "    for index, row in df.iterrows():\n",
        "        if row['data_inversa'] in df_feriado['Data'].values:\n",
        "            df.at[index, 'feriado'] = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3rugaBoAtJc",
        "outputId": "ef4a3039-afd6-434d-cb10-99846c8b5267"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-74175e345fe5>:76: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
            "  df['data_inversa'] = pd.to_datetime(df['data_inversa'])\n",
            "<ipython-input-10-74175e345fe5>:77: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
            "  df_feriado['Data'] = pd.to_datetime(df_feriado['Data'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#features que serão usadas para execução dos modelos\n",
        "\n",
        "# Selecionar as colunas categóricas\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "cols_categoricas = ['uf','condicao_metereologica','delegacia','uop']\n",
        "\n",
        "# Aplicar a codificação LabelEncoder em cada coluna categórica\n",
        "label_encoder = LabelEncoder()\n",
        "for df in dfs:\n",
        "    for col in cols_categoricas:\n",
        "        df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "for df in dfs:\n",
        "    df['regional_int'] = label_encoder.fit_transform(df['regional'])\n",
        "\n",
        "colunas = ['km', 'uf', 'br', 'condicao_metereologica', 'regional','regional_int', 'delegacia', 'uop','chuva','dia','mes','ano','feriado', 'dia_semana_int']"
      ],
      "metadata": {
        "id": "6FtthoQsAvXA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base de dados para algoritmos de regressão e classificação\n",
        "treinamento1 = pd.concat([df_2017, df_2018, df_2019, df_2020, df_2021], ignore_index=True) #TRAIN_RAW\n",
        "teste1 = df_2022[colunas] #TEST_RAW\n",
        "\n",
        "#dados para teste\n",
        "treinamento1 = treinamento1[colunas]\n",
        "sr_df_r = treinamento1[treinamento1['regional'] == 'SPRF-DF']\n",
        "sr_df_br_020_r = sr_df_r[sr_df_r['br'] == 20]\n",
        "sr_df_br_040_r = sr_df_r[sr_df_r['br'] == 40]\n",
        "sr_sp_r = treinamento1[treinamento1['regional'] =='SPRF-SP']\n",
        "sr_sp_br_101_r = sr_sp_r[sr_sp_r['br'] == 101]\n",
        "sr_sp_br_116_r = sr_sp_r[sr_sp_r['br'] == 116]\n",
        "sr_sc_r = treinamento1[treinamento1['regional'] =='SPRF-SC']\n",
        "sr_sc_br_101_r = sr_sc_r[sr_sc_r['br'] == 101]\n",
        "sr_sc_br_116_r = sr_sc_r[sr_sc_r['br'] == 116]\n",
        "\n",
        "#dados para teste\n",
        "sr_df_t_r = teste1[teste1['regional'] == 'SPRF-DF']\n",
        "sr_df_br_020_t_r = sr_df_t_r[sr_df_t_r['br'] == 20]\n",
        "sr_df_br_040_t_r = sr_df_t_r[sr_df_t_r['br'] == 40]\n",
        "sr_sp_t_r = teste1[teste1['regional'] =='SPRF-SP']\n",
        "sr_sp_br_101_t_r = sr_sp_t_r[sr_sp_t_r['br'] == 101]\n",
        "sr_sp_br_116_t_r = sr_sp_t_r[sr_sp_t_r['br'] == 116]\n",
        "sr_sc_t_r = teste1[teste1['regional'] =='SPRF-SC']\n",
        "sr_sc_br_101_t_r = sr_sc_t_r[sr_sc_t_r['br'] == 101]\n",
        "sr_sc_br_116_t_r = sr_sc_t_r[sr_sc_t_r['br'] == 116]\n",
        "\n",
        "regionais = teste1['regional'].unique()\n",
        "\n",
        "#bases de dados que serão utilizadas\n",
        "df_treinamento_r = [sr_df_r, sr_df_br_020_r, sr_df_br_040_r, sr_sp_r, sr_sp_br_101_r,\n",
        "                  sr_sp_br_116_r, sr_sc_r, sr_sc_br_101_r, sr_sc_br_116_r]\n",
        "df_teste_r = [sr_df_t_r, sr_df_br_020_t_r, sr_df_br_040_t_r,sr_sp_t_r, sr_sp_br_101_t_r,\n",
        "            sr_sp_br_116_t_r, sr_sc_t_r, sr_sc_br_101_t_r, sr_sc_br_116_t_r]"
      ],
      "metadata": {
        "id": "X9rpN21QAxUp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODELO DE ML DE CLASSIFICAÇÃO\n",
        "\n",
        "**Bases de dados que serão utilizadas**\n",
        "\n",
        "O base de dados de treinamento é composta pelos registros dos acidentes ocorridos nas rodovias federais brasilerias a partir do ano de 2017 até o ano de 2021\n",
        "> df_treinamento_r = [sr_df_r, sr_df_br_020_r, sr_df_br_040_r, sr_sp_r, sr_sp_br_101_r,sr_sp_br_116_r, sr_sc_r, sr_sc_br_101_r, sr_sc_br_116_r]\n",
        "\n",
        "O base de dados de teste é composta pelos registros dos acidentes ocorridos nas rodovias federais brasilerias no ano de 2022\n",
        "\n",
        "> df_teste_r = [sr_df_t_r, sr_df_br_020_t_r, sr_df_br_040_t_r,sr_sp_t_r, sr_sp_br_101_t_r, sr_sp_br_116_t_r, sr_sc_t_r, sr_sc_br_101_t_r, sr_sc_br_116_t_r]"
      ],
      "metadata": {
        "id": "Ga7Q4fHsBUZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def executar_modelos_classificacao(bases_treinamento, bases_teste):\n",
        "    modelos = {\n",
        "        \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
        "        \"Linear SVC\": LinearSVC(max_iter=1000, tol=1e-5),\n",
        "        \"Bagging Classifier\": BaggingClassifier(),\n",
        "        \"SVC\": SVC(),\n",
        "        \"Random Forest Classifier\": RandomForestClassifier(),\n",
        "        \"Logistic Regression\": LogisticRegression(solver='saga', max_iter=1000, C=0.1),\n",
        "        \"AdaBoost Classifier\": AdaBoostClassifier()\n",
        "    }\n",
        "\n",
        "    scaler = StandardScaler()  # Criar um objeto scaler uma vez\n",
        "\n",
        "    for i, (base_treinamento, base_teste) in enumerate(zip(bases_treinamento, bases_teste)):\n",
        "        print(f\"\\nExecutando modelos para a base de treinamento {i + 1}:\")\n",
        "        imprimir_nome_base(base_treinamento)\n",
        "\n",
        "        X_train = base_treinamento.drop(columns=['km', 'regional'])\n",
        "        y_train = base_treinamento['km']\n",
        "\n",
        "        X_test = base_teste.drop(columns=['km', 'regional'])\n",
        "        y_test = base_teste['km']\n",
        "\n",
        "        # Escalar os dados de treinamento\n",
        "        scaler.fit(X_train)\n",
        "        X_train_scaled = scaler.transform(X_train)\n",
        "\n",
        "        # Escalar os dados de teste usando o mesmo scaler\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        for nome_modelo, modelo in modelos.items():\n",
        "            print(f\"\\n{nome_modelo}:\")\n",
        "            modelo.fit(X_train_scaled, y_train)  # Usar dados de treinamento escalados\n",
        "            y_pred = modelo.predict(X_test_scaled)  # Usar dados de teste escalados\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            print(f\"Acurácia: {accuracy}\")\n",
        "\n",
        "# Função para imprimir o nome da base de dados\n",
        "def imprimir_nome_base(df):\n",
        "    base_name = [name for name, var in globals().items() if var is df][0]\n",
        "    print(f\"Base de dados: {base_name}\")\n",
        "\n",
        "# Exemplo de uso:\n",
        "executar_modelos_classificacao(df_treinamento_r, df_teste_r)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GDvW4UsA-76",
        "outputId": "801474bf-852b-438f-906c-36f03b878eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Executando modelos para a base de treinamento 1:\n",
            "Base de dados: sr_df_r\n",
            "\n",
            "Decision Tree Classifier:\n",
            "Acurácia: 0.04665203073545554\n",
            "\n",
            "Linear SVC:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.06750823271130625\n",
            "\n",
            "Bagging Classifier:\n",
            "Acurácia: 0.05378704720087816\n",
            "\n",
            "SVC:\n",
            "Acurácia: 0.07354555433589462\n",
            "\n",
            "Random Forest Classifier:\n",
            "Acurácia: 0.052689352360043906\n",
            "\n",
            "Logistic Regression:\n",
            "Acurácia: 0.05982436882546652\n",
            "\n",
            "AdaBoost Classifier:\n",
            "Acurácia: 0.06092206366630077\n",
            "\n",
            "Executando modelos para a base de treinamento 2:\n",
            "Base de dados: sr_df_br_020_r\n",
            "\n",
            "Decision Tree Classifier:\n",
            "Acurácia: 0.029166666666666667\n",
            "\n",
            "Linear SVC:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.052083333333333336\n",
            "\n",
            "Bagging Classifier:\n",
            "Acurácia: 0.04375\n",
            "\n",
            "SVC:\n",
            "Acurácia: 0.0375\n",
            "\n",
            "Random Forest Classifier:\n",
            "Acurácia: 0.05\n",
            "\n",
            "Logistic Regression:\n",
            "Acurácia: 0.020833333333333332\n",
            "\n",
            "AdaBoost Classifier:\n",
            "Acurácia: 0.004166666666666667\n",
            "\n",
            "Executando modelos para a base de treinamento 3:\n",
            "Base de dados: sr_df_br_040_r\n",
            "\n",
            "Decision Tree Classifier:\n",
            "Acurácia: 0.07651245551601424\n",
            "\n",
            "Linear SVC:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.10676156583629894\n",
            "\n",
            "Bagging Classifier:\n",
            "Acurácia: 0.06405693950177936\n",
            "\n",
            "SVC:\n",
            "Acurácia: 0.08718861209964412\n",
            "\n",
            "Random Forest Classifier:\n",
            "Acurácia: 0.07473309608540925\n",
            "\n",
            "Logistic Regression:\n",
            "Acurácia: 0.10320284697508897\n",
            "\n",
            "AdaBoost Classifier:\n",
            "Acurácia: 0.005338078291814947\n",
            "\n",
            "Executando modelos para a base de treinamento 4:\n",
            "Base de dados: sr_sp_r\n",
            "\n",
            "Decision Tree Classifier:\n",
            "Acurácia: 0.027274810909924365\n",
            "\n",
            "Linear SVC:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.043089617235846894\n",
            "\n",
            "Bagging Classifier:\n",
            "Acurácia: 0.022461608984643593\n",
            "\n",
            "SVC:\n",
            "Acurácia: 0.00412560165024066\n",
            "\n",
            "Random Forest Classifier:\n"
          ]
        }
      ]
    }
  ]
}