{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAZ55N8JdqWwMkpCNNoKGV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srdiegorodrigues/projeto/blob/main/ann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFIsiEyP-dlu",
        "outputId": "05a5be24-e040-442f-ff04-6200b6df234f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importação das bibliotecas\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "icCfB-qg-0Ck"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Carregar arquivos a serem analisados\n",
        "url = \"/content/drive/MyDrive/MESTRADO/DADOS/OCORRENCIAS/\"\n",
        "\n",
        "df_2017 = pd.read_csv(io.StringIO(open(f'{url}datatran2017.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
        "df_2018 = pd.read_csv(io.StringIO(open(f'{url}datatran2018.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
        "df_2019 = pd.read_csv(io.StringIO(open(f'{url}datatran2019.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
        "df_2020 = pd.read_csv(io.StringIO(open(f'{url}datatran2020.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
        "df_2021 = pd.read_csv(io.StringIO(open(f'{url}datatran2021.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
        "df_2022 = pd.read_csv(io.StringIO(open(f'{url}datatran2022.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
        "\n",
        "df_2017 = df_2017.dropna()\n",
        "df_2018 = df_2018.dropna()\n",
        "df_2019 = df_2019.dropna()\n",
        "df_2020 = df_2020.dropna()\n",
        "df_2021 = df_2021.dropna()\n",
        "df_2022 = df_2022.dropna()\n",
        "\n",
        "dfs = [df_2017, df_2018, df_2019, df_2020, df_2021, df_2022]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9JuzJIf-2U6",
        "outputId": "a4eafd64-81d1-4e71-da3c-f56e3312ee9f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-5849736c2be4>:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_2017 = pd.read_csv(io.StringIO(open(f'{url}datatran2017.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
            "<ipython-input-10-5849736c2be4>:5: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_2018 = pd.read_csv(io.StringIO(open(f'{url}datatran2018.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
            "<ipython-input-10-5849736c2be4>:6: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_2019 = pd.read_csv(io.StringIO(open(f'{url}datatran2019.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
            "<ipython-input-10-5849736c2be4>:7: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_2020 = pd.read_csv(io.StringIO(open(f'{url}datatran2020.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
            "<ipython-input-10-5849736c2be4>:8: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_2021 = pd.read_csv(io.StringIO(open(f'{url}datatran2021.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n",
            "<ipython-input-10-5849736c2be4>:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_2022 = pd.read_csv(io.StringIO(open(f'{url}datatran2022.csv', 'r', encoding='ISO-8859-1').read()), sep=';', error_bad_lines =False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#carregar base de dados dos feriados nacionais\n",
        "df_feriado = pd.read_csv(\"/content/drive/MyDrive/MESTRADO/DADOS/OCORRENCIAS/feriados_nacionais.csv\",  encoding='ISO-8859-1', sep=\";\")\n",
        "\n",
        "#arredondar o valor de km para cima ou para baixo, conforme o valor após o ponto\n",
        "def arredondar_km(valor):\n",
        "    valor_float = float(valor)  # Converter para float\n",
        "    inteiro = int(valor_float)  # parte inteira do valor\n",
        "    decimal = valor_float - inteiro  # parte decimal do valor\n",
        "\n",
        "    # Se o valor decimal for maior ou igual a 0.5, arredonda para cima, caso contrário, arredonda para baixo\n",
        "    if decimal >= 0.5:\n",
        "        return inteiro + 1\n",
        "    else:\n",
        "        return inteiro\n",
        "\n",
        "#converter o valor da feature br para int\n",
        "def br_int(valor):\n",
        "    valor_float = float(valor)\n",
        "    valor_int = int(valor_float)\n",
        "    return valor_int\n",
        "\n",
        "dias_semana_para_int = {\n",
        "        'segunda-feira': 0,\n",
        "        'terça-feira': 1,\n",
        "        'quarta-feira': 2,\n",
        "        'quinta-feira': 3,\n",
        "        'sexta-feira': 4,\n",
        "        'sábado': 5,\n",
        "        'domingo': 6\n",
        "    }\n",
        "\n",
        "def converter_dia_semana_para_int(dia):\n",
        "    return dias_semana_para_int[dia]\n",
        "\n",
        "for df in dfs:\n",
        "    #alterar condições metereologicas para evitar outliers\n",
        "    df['condicao_metereologica'] = df['condicao_metereologica'].str.replace('Ignorado', 'Céu Claro')\n",
        "    df['condicao_metereologica'] = df['condicao_metereologica'].str.replace('Neve', 'Chuva')\n",
        "\n",
        "    #substituir / por - na data_inversa\n",
        "    df['data_inversa'] = df['data_inversa'].str.replace('/', '-')\n",
        "\n",
        "    #criação da feature chuva para determinar se a pista estava molhada na hora do acidente\n",
        "    for indice, item in df.iterrows():\n",
        "        if item['condicao_metereologica'] in ['Chuva', 'Garoa/Chuvisco', 'Granizo']:\n",
        "            df.at[indice, 'chuva'] = 1\n",
        "        else:\n",
        "            df.at[indice, 'chuva'] = 0\n",
        "\n",
        "    #fragmentação da feature data_inversa em dia, mes e ano\n",
        "    df[['dia', 'mes', 'ano']] = df['data_inversa'].apply(lambda x: pd.Series(x.split('-')).astype(int))\n",
        "\n",
        "    # #Concatenar colunas data_inversa e horario e criando a feature data_hora\n",
        "    df['data_hora'] = df.apply(lambda x: '%s-%s' % (x['data_inversa'],x['horario']), axis=1)\n",
        "    #Criação da feature data_hora_int\n",
        "    df['data_hora'] = pd.to_datetime(df['data_hora'])\n",
        "    # Convertendo as datas para timestamps Unix\n",
        "    df['data_hora_int'] = df['data_hora'].astype(int) // 10**9\n",
        "\n",
        "    #normalizar a feature data_hora_int\n",
        "    scaler = MinMaxScaler()\n",
        "    df['data_hora_int'] = scaler.fit_transform(df['data_hora_int'].values.reshape(-1, 1))\n",
        "\n",
        "    #substituir a , por . na feature km\n",
        "    df['km'] = df['km'].str.replace(',','.')\n",
        "    df['km'] = df['km'].apply(arredondar_km)\n",
        "    df['br'] = df['br'].apply(br_int)\n",
        "\n",
        "\n",
        "    df['dia_semana_int'] =  df['dia_semana'].apply(converter_dia_semana_para_int)\n",
        "\n",
        "    #Setar se dia da ocorrência do acidente era feriado ou não\n",
        "    #Criando a feature feriado com base nas portarias do governo federal\n",
        "    #Caso seja feriado, será atribuido o valor 1 para o registro\n",
        "    #caso contrário, será atribuido o valor 0\n",
        "    df['data_inversa'] = pd.to_datetime(df['data_inversa'])\n",
        "    df_feriado['Data'] = pd.to_datetime(df_feriado['Data'])\n",
        "    df_feriado['Data'] = df_feriado['Data'].sort_values(ascending=False).values\n",
        "    # Criando uma nova coluna 'feriado' no DataFrame principal e inicializando com 0\n",
        "    df['feriado'] = 0\n",
        "\n",
        "    # Loop for para comparar as datas e atribuir 1 se forem iguais\n",
        "    for index, row in df.iterrows():\n",
        "        if row['data_inversa'] in df_feriado['Data'].values:\n",
        "            df.at[index, 'feriado'] = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKNVbYqc-5IA",
        "outputId": "2096dbbb-d7e5-46be-9d5e-c72e35670590"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-74175e345fe5>:76: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
            "  df['data_inversa'] = pd.to_datetime(df['data_inversa'])\n",
            "<ipython-input-11-74175e345fe5>:77: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
            "  df_feriado['Data'] = pd.to_datetime(df_feriado['Data'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#features que serão usadas para execução dos modelos\n",
        "\n",
        "# Selecionar as colunas categóricas\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "cols_categoricas = ['uf','condicao_metereologica','delegacia','uop']\n",
        "\n",
        "# Aplicar a codificação LabelEncoder em cada coluna categórica\n",
        "label_encoder = LabelEncoder()\n",
        "for df in dfs:\n",
        "    for col in cols_categoricas:\n",
        "        df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "for df in dfs:\n",
        "    df['regional_int'] = label_encoder.fit_transform(df['regional'])\n",
        "\n",
        "colunas = ['km', 'uf', 'br', 'condicao_metereologica', 'regional','regional_int', 'delegacia', 'uop','chuva','dia','mes','ano','feriado', 'dia_semana_int']"
      ],
      "metadata": {
        "id": "3GDYLbW2--4w"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base de dados para algoritmos ANN, Gradiente Boosting Regressor\n",
        "#treino anos 2017, 2018, 2019, 2020\n",
        "#validação 2021\n",
        "#teste 2022\n",
        "treinamento2 = pd.concat([df_2017, df_2018, df_2019, df_2020], ignore_index=True)\n",
        "treinamento2 = treinamento2[colunas]\n",
        "teste2 = df_2021[colunas]\n",
        "validacao2 = df_2022[colunas]\n",
        "\n",
        "#dados para teste\n",
        "treinamento2 = treinamento2[colunas]\n",
        "sr_df = treinamento2[treinamento2['regional'] == 'SPRF-DF']\n",
        "sr_df_br_020 = sr_df[sr_df['br'] == 20]\n",
        "sr_df_br_040 = sr_df[sr_df['br'] == 40]\n",
        "sr_sp = treinamento2[treinamento2['regional'] =='SPRF-SP']\n",
        "sr_sp_br_101 = sr_sp[sr_sp['br'] == 101]\n",
        "sr_sp_br_116 = sr_sp[sr_sp['br'] == 116]\n",
        "sr_sc = treinamento2[treinamento2['regional'] =='SPRF-SC']\n",
        "sr_sc_br_101 = sr_sc[sr_sc['br'] == 101]\n",
        "sr_sc_br_116 = sr_sc[sr_sc['br'] == 116]\n",
        "\n",
        "#dados para teste\n",
        "sr_df_t = teste2[teste2['regional'] == 'SPRF-DF']\n",
        "sr_df_br_020_t = sr_df_t[sr_df_t['br'] == 20]\n",
        "sr_df_br_040_t = sr_df_t[sr_df_t['br'] == 40]\n",
        "sr_sp_t = teste2[teste2['regional'] =='SPRF-SP']\n",
        "sr_sp_br_101_t = sr_sp_t[sr_sp_t['br'] == 101]\n",
        "sr_sp_br_116_t = sr_sp_t[sr_sp_t['br'] == 116]\n",
        "sr_sc_t = teste2[teste2['regional'] =='SPRF-SC']\n",
        "sr_sc_br_101_t = sr_sc_t[sr_sc_t['br'] == 101]\n",
        "sr_sc_br_116_t = sr_sc_t[sr_sc_t['br'] == 116]\n",
        "\n",
        "#dados para validação\n",
        "sr_df_v = validacao2[validacao2['regional'] == 'SPRF-DF']\n",
        "sr_df_br_020_v = sr_df_v[sr_df_v['br'] == 20]\n",
        "sr_df_br_040_v = sr_df_v[sr_df_v['br'] == 40]\n",
        "sr_sp_v = validacao2[validacao2['regional'] =='SPRF-SP']\n",
        "sr_sp_br_101_v = sr_sp_v[sr_sp_v['br'] == 101]\n",
        "sr_sp_br_116_v = sr_sp_v[sr_sp_v['br'] == 116]\n",
        "sr_sc_v = validacao2[validacao2['regional'] =='SPRF-SC']\n",
        "sr_sc_br_101_v = sr_sc_v[sr_sc_v['br'] == 101]\n",
        "sr_sc_br_116_v = sr_sc_v[sr_sc_v['br'] == 116]\n",
        "\n",
        "regionais = teste2['regional'].unique()"
      ],
      "metadata": {
        "id": "MsnDkJd8_B_h"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bases de dados que serão utilizadas\n",
        "df_treinamento = [sr_df, sr_df_br_020, sr_df_br_040, sr_sp, sr_sp_br_101,\n",
        "                  sr_sp_br_116, sr_sc, sr_sc_br_101, sr_sc_br_116]\n",
        "df_validacao =  [sr_df_v, sr_df_br_020_v, sr_df_br_040_v, sr_sp_v, sr_sp_br_101_v,\n",
        "             sr_sp_br_116_v, sr_sc_v, sr_sc_br_101_v, sr_sc_br_116_v]\n",
        "df_teste = [sr_df_t, sr_df_br_020_t, sr_df_br_040_t,sr_sp_t, sr_sp_br_101_t,\n",
        "            sr_sp_br_116_t, sr_sc_t, sr_sc_br_101_t, sr_sc_br_116_t]"
      ],
      "metadata": {
        "id": "zne1QGyVCTtM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODELOS UTILIZANDO REDES NEURAIS ARTIFICIAIS\n",
        "\n",
        "\n",
        "\n",
        "**Bases de dados que serão utilizadas**\n",
        "\n",
        "O base de dados de treinamento é composta pelos registros dos acidentes ocorridos nas rodovias federais brasilerias a partir do ano de 2017 até o ano de 2020\n",
        "\n",
        "\n",
        "> df_treinamento = [sr_df, sr_df_br_020, sr_df_br_040, sr_sp, sr_sp_br_101,\n",
        "                  sr_sp_br_116, sr_sc, sr_sc_br_101, sr_sc_br_116]\n",
        "\n",
        "\n",
        "\n",
        "O base de dados de teste é composta pelos registros dos acidentes ocorridos nas rodovias federais brasilerias no ano de 2021\n",
        "\n",
        "\n",
        ">df_teste = [sr_df_t, sr_df_br_020_t, sr_df_br_040_t,sr_sp_t, sr_sp_br_101_t,\n",
        "            sr_sp_br_116_t, sr_sc_t, sr_sc_br_101_t, sr_sc_br_116_t]\n",
        "\n",
        "\n",
        "\n",
        "O base de dados de validação é composta pelos registros dos acidentes ocorridos nas rodovias federais brasilerias no ano de 2022\n",
        "\n",
        "\n",
        "> df_validacao =  [sr_df_v, sr_df_br_020_v, sr_df_br_040_v, sr_sp_v, sr_sp_br_101_v,sr_sp_br_116_v, sr_sc_v, sr_sc_br_101_v, sr_sc_br_116_v]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rk0GyCds_GOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avaliar_modelos(bases_treinamento, bases_teste, bases_validacao):\n",
        "    for i, (base_treinamento, base_teste, base_validacao) in enumerate(zip(bases_treinamento, bases_teste, bases_validacao)):\n",
        "        def imprimir_nome_base(df):\n",
        "            base_name = [name for name, var in globals().items() if var is df][0]\n",
        "            print('\\n')\n",
        "            print(f\"Executando modelo com a base de treinamento: {base_name}\")\n",
        "\n",
        "        # Separar as características (X) e o alvo (y)\n",
        "        imprimir_nome_base(base_treinamento)\n",
        "        X_train = base_treinamento.drop(columns=['km','regional'])\n",
        "        y_train = base_treinamento['km']\n",
        "\n",
        "        X_test = base_teste.drop(columns=['km','regional'])\n",
        "        y_test = base_teste['km']\n",
        "\n",
        "        X_val = base_validacao.drop(columns=['km','regional'])\n",
        "        y_val = base_validacao['km']\n",
        "\n",
        "        # Normalizar os dados\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "        # Modelo de rede neural\n",
        "        modelo = Sequential([\n",
        "            Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "            Dropout(0.5),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "        # Compilação do modelo\n",
        "        modelo.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error', 'mae'])\n",
        "\n",
        "        # Treinamento do modelo\n",
        "        modelo.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
        "\n",
        "        # Avaliação do modelo\n",
        "        loss, mse, mae = modelo.evaluate(X_val_scaled, y_val)\n",
        "\n",
        "        # Fazendo previsões\n",
        "        y_pred = modelo.predict(X_val_scaled)\n",
        "\n",
        "        # Calculando o erro médio quadrado (MSE)\n",
        "        mse = mean_squared_error(y_val, y_pred)\n",
        "\n",
        "        # Calculando o R² Score\n",
        "        r2 = r2_score(y_val, y_pred)\n",
        "\n",
        "        # Calculando o RMSE em quilômetros\n",
        "        rmse_km = mean_squared_error(y_val, y_pred, squared=False)\n",
        "\n",
        "        print(f\"MSE: {mse}\")\n",
        "        print(f\"R² Score: {r2}\")\n",
        "        print(f\"RMSE (km): {rmse_km}\")\n",
        "\n",
        "# Exemplo de uso:\n",
        "avaliar_modelos(df_treinamento, df_teste, df_validacao)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0PaGjRU_MSJ",
        "outputId": "396ff526-6f38-4680-9eca-83679a50cc3e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Executando modelo com a base de treinamento: sr_df\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 409.5483 - mean_squared_error: 409.5483 - mae: 12.0510\n",
            "57/57 [==============================] - 0s 2ms/step\n",
            "MSE: 409.5483599735744\n",
            "R² Score: 0.7262722161558336\n",
            "RMSE (km): 20.23730120281789\n",
            "\n",
            "\n",
            "Executando modelo com a base de treinamento: sr_df_br_020\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 1093.2250 - mean_squared_error: 1093.2250 - mae: 27.9577\n",
            "15/15 [==============================] - 0s 2ms/step\n",
            "MSE: 1093.225136919385\n",
            "R² Score: 0.6578529867783541\n",
            "RMSE (km): 33.063955252198504\n",
            "\n",
            "\n",
            "Executando modelo com a base de treinamento: sr_df_br_040\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 289.8994 - mean_squared_error: 289.8994 - mae: 9.2673\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "MSE: 289.8993903023438\n",
            "R² Score: 0.6755470609815992\n",
            "RMSE (km): 17.02643210723679\n",
            "\n",
            "\n",
            "Executando modelo com a base de treinamento: sr_sp\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 15314.0039 - mean_squared_error: 15314.0039 - mae: 97.4241\n",
            "137/137 [==============================] - 0s 2ms/step\n",
            "MSE: 15314.003531923972\n",
            "R² Score: 0.17479937344380936\n",
            "RMSE (km): 123.749761744918\n",
            "\n",
            "\n",
            "Executando modelo com a base de treinamento: sr_sp_br_101\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 17879.6465 - mean_squared_error: 17879.6465 - mae: 132.9369\n",
            "6/6 [==============================] - 0s 4ms/step\n",
            "MSE: 17879.64796784201\n",
            "R² Score: -55.7568470869126\n",
            "RMSE (km): 133.7148008555598\n",
            "\n",
            "\n",
            "Executando modelo com a base de treinamento: sr_sp_br_116\n",
            "99/99 [==============================] - 0s 2ms/step - loss: 17969.2090 - mean_squared_error: 17969.2090 - mae: 111.9047\n",
            "99/99 [==============================] - 0s 2ms/step\n",
            "MSE: 17969.205558505117\n",
            "R² Score: -0.07919614300222699\n",
            "RMSE (km): 134.0492654157609\n",
            "\n",
            "\n",
            "Executando modelo com a base de treinamento: sr_sc\n",
            "238/238 [==============================] - 0s 2ms/step - loss: 11061.6855 - mean_squared_error: 11061.6855 - mae: 80.8916\n",
            "238/238 [==============================] - 0s 2ms/step\n",
            "MSE: 11061.683395965376\n",
            "R² Score: 0.3940420183391842\n",
            "RMSE (km): 105.17453777395637\n",
            "\n",
            "\n",
            "Executando modelo com a base de treinamento: sr_sc_br_101\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 8677.8350 - mean_squared_error: 8677.8350 - mae: 81.2355\n",
            "123/123 [==============================] - 0s 2ms/step\n",
            "MSE: 8677.835325418553\n",
            "R² Score: 0.15779816448978568\n",
            "RMSE (km): 93.15489963184199\n",
            "\n",
            "\n",
            "Executando modelo com a base de treinamento: sr_sc_br_116\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 7737.9321 - mean_squared_error: 7737.9316 - mae: 76.4091\n",
            "12/12 [==============================] - 0s 2ms/step\n",
            "MSE: 7737.931105955\n",
            "R² Score: 0.019270330548847747\n",
            "RMSE (km): 87.96551088895579\n"
          ]
        }
      ]
    }
  ]
}